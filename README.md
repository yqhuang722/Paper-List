# Reading List
- NeurIPS 2024, `GRL` Revisiting the Message Passing in Heterophilous Graph Neural Networks, <u>[website](https://openreview.net/forum?id=7g8WSOHJtP)</u>, <u>[arXiv](https://arxiv.org/abs/2405.17768)</u>
- NeurIPS 2024, `GNN` Graph Neural Networks Need Cluster-Normalize-Activate Modules, <u>[website](https://openreview.net/forum?id=faj2EBhdHC)</u>
- CIKM 2024, `GRL` Spectral-Aware Augmentation for Enhanced Graph Representation Learning, <u>[arXiv](https://arxiv.org/abs/2310.13845)</u>
- NeurIPS 2024, `GFM` GLBench: A Comprehensive Benchmark for Graph with Large Language Models, <u>[arXiv](https://arxiv.org/abs/2407.07457)</u>
- NeurIPS 2024, `GFM` Text-space Graph Foundation Models: Comprehensive Benchmarks and New Insights, <u>[arXiv](https://arxiv.org/abs/2406.10727)</u>
- KDD 2023, `GR` Interpretable Sparsification of Brain Graphs: Better Practices and Effective Designs for Graph Neural Networks, <u>[arXiv](https://arxiv.org/abs/2306.14375)</u>
- IJCAI 2024, `GR` A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation, <u>[arXiv](https://arxiv.org/abs/2402.03358)</u>
- KDD-Exp 2023, `LLM+Graph` Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs, <u>[arXiv](https://arxiv.org/abs/2307.03393)</u>
- NeurIPS-WS 2023, `LLM+Graph` Can LLMs Effectively Leverage Graph Structural Information: When and Why, <u>[website](https://openreview.net/forum?id=jyfiPivRBH)</u>


# Recent Read Papers
- NeurIPS 2024, `GNN` Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification, <u>[arXiv](https://arxiv.org/html/2406.08993)</u>
- WWW 2025, `GNN` Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection, <u>[website](https://openreview.net/forum?id=6B6AmBaWfv#discussion)</u>
- ICLR 2025, `GP` Edge Prompt Tuning for Graph Neural Networks, <u>[arXiv](https://arxiv.org/abs/2503.00750)</u>
- ICML 2024, `LLM+Graph` LLaGA: Large Language and Graph Assistant, <u>[arXiv](https://arxiv.org/abs/2402.08170)</u>
- SIGIR 2024, `LLM+Graph` GraphGPT: Graph Instruction Tuning for Large Language Models, <u>[arXiv](https://arxiv.org/abs/2310.13023)</u>
- NeurIPS 2024, `LLM+Graph` LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings, <u>[website](https://proceedings.neurips.cc/paper_files/paper/2024/hash/0b77d3a82b59e9d9899370b378087faf-Abstract-Conference.html)</u>
- ICLR 2024, `LLM+Graph` One For All: Towards Training One Graph Model For All Classification Tasks, <u>[website](https://openreview.net/forum?id=4IT2pgc9v6)</u>
- WWW 2025, `GFM` GraphCLIP: Enhancing Transferability in Graph Foundation  Models for Text-Attributed Graphs, <u>[arXiv](https://arxiv.org/abs/2502.05424)</u> / <u>[github](https://github.com/ZhuYun97/GraphCLIP)</u>
- NeurIPS 2023, `GSL` OpenGSL: A Comprehensive Benchmark for Graph Structure Learning, <u>[website](https://papers.nips.cc/paper_files/paper/2023/hash/39f8ef62e061042cca8c8f46d7e0e31b-Abstract-Datasets_and_Benchmarks.html)</u> / <u>[github](https://github.com/OpenGSL/OpenGSL)</u>
- WWW 2022, `GSL` Towards unsupervised deep graph structure learning, <u>[website](https://dl.acm.org/doi/10.1145/3485447.3512186)</u> / <u>[github](https://github.com/TrustAGI-Lab/SUBLIME/tree/main)</u>
- NeurIPS 2023, `GSL` GSLB: The Graph Structure Learning Benchmark, <u>[website](https://proceedings.neurips.cc/paper_files/paper/2023/hash/60bc87f3cf5257579435d92ec12c761b-Abstract-Datasets_and_Benchmarks.html)</u> / <u>[github](https://github.com/GSL-Benchmark/GSLB)</u>
- arXiv 2025.02, `GFM` SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation, <u>[arXiv](https://arxiv.org/abs/2502.05424)</u>
- arXiv 2025.01, `GFM` Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment, <u>[arXiv](https://arxiv.org/abs/2502.02017#:~:text=To%20address%20these%20issues%2C%20we%20propose%20the%20Multi-Domain,leverages%20cross-domain%20topological%20information%20to%20facilitate%20robust%20knowledg)</u>
- arXiv 2025.02, `GFM` Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees, <u>[arXiv](https://arxiv.org/abs/2412.16441)</u>

> Begin on February 27, 2025.

